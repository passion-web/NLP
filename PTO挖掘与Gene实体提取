# -*- coding: utf-8 -*-
"""
Created on Tue May 25 17:38:27 2021

@author: SunZichun
"""
import re
from nltk.tokenize import sent_tokenize
from nltk.tokenize import word_tokenize
from collections import defaultdict
from nltk import pos_tag
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
import nltk
import os
import re
import pandas as pd
import string

def read_pto(pto_file):
    """
    读取PTO文件 存为字典
    key: pto term id
    value: name and synonym
    :param pto_file: PTO 文件
    :return: 字典
    """
    opt_dic = defaultdict(list)  #存储PTOid，name，sym（近义词）。一个位置存放三个地方
    count_term = 0  #PTO数量
    count_word = 0  #包括名字和其近义词的数量
    with open(file=pto_file,mode='r',encoding="utf-8") as f:
        for line in f:
            l = line.strip()
            if l.startswith('id'): #id开头的话，存储
                count_term += 1
                id = l.split()[1]  #去除PTOid
            if l.startswith('name'):
                # 读取名字
                count_word += 1
                name = ' '.join(l.split(' ')[1:])
                if '(' in name:
                    name = name[:name.find('(') - 1 ]  #取出name，去除（）
                opt_dic[id].append(name)
            if l.startswith('synonym'):
                # 正则匹配读取同义名
                count_word += 1
                pattern = re.compile('"(.*)"')
                synonym = pattern.findall(l)[ 0 ]   #讲列表转为字符串

                pattern_ba = re.compile(r'[(].*?[)]')  #取出括号内容
                backets = pattern_ba.findall(l)
                if synonym.startswith('('):  #讲除第一个之后的括号都删除
                    for ba in backets[ 1: ]:
                        synonym = synonym.replace(ba, '')
                        synonym = synonym.strip(' ')
                else:  #如果开头没有括号，则所有括号都删除
                    for ba in backets:
                        synonym = synonym.replace(ba, '')
                        synonym = synonym.strip(' ')
                opt_dic[id].append(synonym)
    print('一共 {0} terms, 包含 {1} 个名字及同义名.'.format(count_term, count_word))
    del opt_dic['part_of']
    return opt_dic

if __name__ == '__main__':
    pto_file = 'F:/to-basic.obo'
    opt_dic1 = read_pto(pto_file)
    

with open('F:/result_OZ_Pubtator.txt') as txt:
    textout = ''
    abstract = ''
    data = []
    alllist = []
    for line in txt:
        if re.match(r'^\d{8}\|t\|', line):
            sublist = []
            head = re.match(r'^\d{8}\|t\|', line).group()
            pmid = head.rstrip('\|t\|')
            sublist.append(pmid)
            title = line.lstrip(head).rstrip('\n')
            sublist.append(title)
            text = line.lstrip(head).rstrip('\n') + ' '
            textout += line
        if re.match(r'^\d{8}\|a\|', line):
            head = re.match(r'^\d{8}\|a\|', line).group()
            #print(head)
            text += line.lstrip(head)
            textout += line + '\n'
            abstract = line.lstrip(head).rstrip('\n')
            sublist.append(abstract)
            alllist.append(sublist)
        if re.match(r'^\d{8}\t', line):
            data.append(line.rstrip().split('\t'))
    df = pd.DataFrame(data)
    df_gene = df.loc[df[4] == 'Gene']

count = 0
count_result = 0
out_file = 'F:/result_fen.txt'
wf = open(out_file,'w')
wf.write('PTO\tsentence\ttile\tterm\n')

    
for ta in alllist:
    count += 1
    pmid = ta[0]
    abs = ta[2]
    sent_list = [i for i in sent_tokenize(abs)]
    for sent in sent_list:
        for term,name_list in opt_dic1.items():
            for name in name_list:
                if name in sent:
                    wf.write('{0}\t{1}\t{2}\t{3}\n'. \
                             format(term, sent, pmid, '|'.join(name_list)))
                    count_result += 1 

wf.close()
